# CAP
* 一致性（Consistency） : 所有节点访问同一份最新的数据副本
* 可用性（Availability）: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。
* 分区容错性（Partition Tolerance） : 分布式系统出现网络分区的时候，仍然能够对外提供服务。
##常见框架
* ZooKeeper 保证的是 CP， Eureka 保证的则是 AP， Nacos 不仅支持 CP 也支持 AP， redis主从保证的是AP
#BASE
* BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。
* 即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。
* AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。
#总结
* 如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”的话，我们要思考如何保证 CA 。
* ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。
#集群
* 在集群中大多数服务器响应，命令就可以完成，不会被少数运行缓慢的服务器来影响整体系统性能
##元数据存储
* 集群，集群往往需要维护一定的元数据，比如实例的ip地址，缓存分片的 slots 信息等，所以需要一套分布式机制来维护元数据的一致性。这类机制一般有两个模式：分散式和集中式
* 散式机制将元数据存储在部分或者所有节点上，不同节点之间进行不断的通信来维护元数据的变更和一致性。
* 而集中式是将集群元数据集中存储在外部节点或者中间件上，比如 zookeeper。旧版本的 kafka 和 storm 等都是使用该模式。
##通信模型-共识算法
* 安全。确保在非拜占庭条件（也就是上文中提到的简易版拜占庭）下的安全性，包括网络延迟、分区、包丢失、复制和重新排序。
* 可用。只要大多数服务器都是可操作的，并且可以相互通信，也可以与客户端进行通信，那么这些服务器就可以看作完全功能可用的。因此，一个典型的由五台服务器组成的集群可以容忍任何两台服务器端故障
* 一致性不依赖时序。错误的时钟和极端的消息延迟，在最坏的情况下也只会造成可用性问题，而不会产生一致性问题。
* 如Paxos 算法，Raft算法，ZAB协议，Gossip 协议
* Paxos 和 Raft 等都需要全部节点或者大多数节点(超过一半)正常运行，整个集群才能稳定运行，而 Gossip 则不需要半数以上的节点运行。
#Paxos 算法
* 分布式系统共识算法，而不是一致性算法
* Paxos 算法主要包含 2 个部分:Basic Paxos 算法和Multi-Paxos 思想。
* Raft 算法、ZAB 协议、 FastPaxos 算法都是基于 Paxos 算法改进而来
##Basic Paxos 算法
* Basic Paxos 中存在 3 个重要的角色：
    * 提议者（Proposer）：也可以叫做协调者（coordinator），同时保证即使多个提议者的提议之间产生了冲突，那么算法都能进行下去；
    * 接受者（Acceptor）：也可以叫做投票员（voter），负责对提议者的提议投票，同时需要记住自己的投票历史；
    * 学习者（Learner）：如果有超过半数接受者就某个提议达成了共识，那么学习者就需要接受这个提议，并就该提议作出运算，然后将运算结果返回给客户端。
* ![](img/base-paxos.png)   
##Multi Paxos 思想 
* Multi-Paxos 只是一种思想，这种思想的核心就是通过多个 Basic Paxos 实例就一系列值达成共识。
* 二阶段提交是达成共识常用的方式，Basic Paxos 就是通过二阶段提交的方式来达成共识。Basic Paxos 还支持容错，少于一般的节点出现故障时，集群也能正常工作。
#Raft算法
##流程
* http://thesecretlivesofdata.com/raft/
* https://www.cnblogs.com/buttercup/p/12903126.html
##角色
* Leader：负责发起心跳，响应客户端，创建日志，同步日志。
* Candidate：Leader 选举过程中的临时角色，由 Follower 转化而来，发起投票参与竞选。
* Follower：接受 Leader 的心跳和日志同步数据，投票给 Candidate
* 正常的情况下，只有一个服务器是 Leader，剩下的服务器是 Follower。Follower 是被动的，它们不会发送任何请求，只是响应来自 Leader 和 Candidate 的请求
##任期
* raft 算法将时间划分为任意长度的任期（term），任期用连续的数字表示，看作当前 term 号。
* 每个节点都会存储当前的 term 号，当服务器之间进行通信时会交换当前的 term 号；如果有服务器发现自己的 term 号比其他人小，那么他会更新到较大的 term 值。如果一个 Candidate 或者 Leader 发现自己的 term 过期了，他会立即退回成 Follower。如果一台服务器收到的请求的 term 号是过期的，那么它会拒绝此次请求。
##日志
* entry：每一个事件成为 entry，只有 Leader 可以创建 entry。entry 的内容为<term,index,cmd>其中 cmd 是可以应用到状态机的操作。
* entry 总是先被 Leader 添加到自己的 log 数组中，然后再发起共识请求，获得同意后才会被 Leader 提交给状态机。
##领导人选举
* raft 使用心跳机制来触发 Leader 的选举。
* leader 会向所有的 Follower 周期性发送心跳来保证自己的 Leader 地位。
* 如果一个 Follower 在一个周期内没有收到心跳信息，就叫做选举超时，开始选出一个新的 Leader。自身变为Candidate。
* 需要保证Leader 永远不会覆盖已经存在的日志条目，所有节点收到投票信息时，会对该 entry 进行比较，如果发现自己的更新，则拒绝投票给该 Candidate。
* 赢得选举的条件是：一个 Candidate 在一个任期内收到了来自集群内的多数选票（N/2+1），就可以成为 Leader
* 同一时刻出现多个 Candidate，导致没有 Candidate 获得大多数选票，可能会无限重复下去
* raft 使用了随机的选举超时时间来避免上述情况
* 在选主期间整个集群对外是不可用的
##日志复制/数据同步
* 当一个数据修改的请求过来，会直接找到 Leader 节点，所有的增删改查都由 Leader 受理。然后同步给各个 Follower。
* 每次数据同步操作同时也是一个心跳，会更新 Follower 的 election timeout。
* 另外只有当多数节点返回同步成功之后，Leader 才会给客户端返回操作成功。
* Follower 上的冲突日志会被领导者的日志覆盖
#分区容错
* 然后是最麻烦的部分，如果出现了网络分区怎么办？比如原本五个节点的集群，被分成了双节点和三节点的两个集群。
  
* 假设原本的 Leader 在双节点的集群里面，那么这个集群会照常运作，单不能接受请求
* 而新出现的三个节点的集群，由于没有收到心跳，会开始选举，然后选出新的 Leader。选出后可以接受请求
* 客户端如果发送到双节点的集群中，Leader 把操作同步给 Follower，会发现收不到足够多的 Follower 响应（因为这个 Follower 还以为自己的集群是五个节点），然后就没办法同步数据。
* 而三节点的新集群，就可以顺利更新数据。
* 如果这时候网络恢复了，各个节点又可以正常通信，三节点集群中的 Leader 和 双节点集群中的 Leader 会互相通信，然后会发现三节点的 Leader 由于一直正常运行，term 值会不断增大，所以大家会采信他的数据。于是双节点的两台机器会回滚，然后全部接受新 Leader 的数据同步。
##中间实现
* 3.0后kafka
#ZAB协议
* https://zhuanlan.zhihu.com/p/147691282
* https://dbaplus.cn/news-141-1875-1.html
##角色
* Leader 也就是领导者
* Follower 也就是接受提议的跟随者
* Observer 可以认为是领导者的的 Copy，不参与投票，在这可以忽略
##流程
* 选举（election）是选出哪台为主机；
* 发现（discovery）、同步（sync）当主选出后，要做的恢复数据的阶段；
* 广播（Broadcast）当主机和从选出并同步好数据后，正常的主写同步从写数据的阶段。
## 消息广播阶段-数据同步
* Leader 节点接受事务提交，并且将新的 Proposal 请求广播给 Follower 节点，收集各个节点的反馈，决定是否进行 Commit，在这个过程中， Quorum 选举机制。
* 写Leader：
    ````
      客户端向Leader发起写请求
      Leader将写请求以Proposal的形式发给所有Follower并等待ACK
      Follower收到Leader的Proposal后返回ACK
      Leader得到过半数的ACK（Leader对自己默认有一个ACK）后向所有的Follower和Observer发送Commmit
      Leader将处理结果返回给客户端
    ````
* 写Leader：
    ````
    Follower/Observer均可接受写请求，但不能直接处理，而需要将写请求转发给Leader处理
    除了多了一步请求转发，其它流程与直接写Leader无任何区别
    ```` 
##崩溃恢复阶段-选举
* 算法：FastLeaderElection（默认算法）
````
  每次选举都要把选举轮数加一，类似于 zxid 里的 epoch 字段，防止不同轮次的选举互相干扰。
  
  每个进入 Looking 状态的节点，会先把投票箱清空，然后通过广播投票给自己，再把投票消息发给其它机器，同时也在接受其他节点的投票。投票信息包括：轮数、被投票节点的 zxid，被投票节点的编号等等。
  
  其他 Looking 状态的节点收到后：
  
  首先判断票是否有效。是否有效的方法为看票的投票轮数和本地记载的投票轮数是否相等：
    如果比本地投票轮数的小，丢弃。
  
  如果比本地投票轮数的大，证明自己投票过期了，清空本地投票信息，更新投票轮数和结果为收到的内容。通知其他所有节点新的投票方案。
  如果和本地投票轮数相等，按照投票的优先级比较收到的选票和自己投出去的选票：
  如果收到的优先级大，则更新自己的投票为对方发过来投票方案，把投票发出去。
  如果收到的优先级小，则忽略该投票。
  如果收到的优先级相等，则更新对应节点的投票。
  每收集到一个投票后，查看已经收到的投票结果记录列表，看是否有节点能够达到一半以上的投票数。如果有达到，则终止投票，宣布选举结束，更新自身状态。然后进行发现和同步阶段。否则继续收集投票。
  投票终止后，服务器开始更新自身状态。若过半的票投给了自己，则将自己的服务器状态更新为 Leading，否则将自己的状态更新为 Following。
````
##分区容错
* 目前 ZooKeeper 和 Raft 都是过半即可，所以对于分区是容忍的。
* 如5台机器，分区发生后分成 2 部分，一部分 3 台，另一部分 2 台，这 2 部分之间无法相互通信。
* 含有 3 台的那部分，仍然可以凑成一个过半，仍然可以对外提供服务。
* 含有 2 台的那部分，则无法提供服务，即只要连接的是这 2 台机器，都无法执行相关请求。
##中间件
* zookeeper
#Gossip 协议
* Gossip 协议：在一个处于有界网络的集群里，如果每个节点都随机与其他节点交换特定信息，经过足够长的时间后，集群各个节点对该份信息的认知终将收敛到一致。
* Gossip协议是完全符合 BASE 原则，可以用在任何要求最终一致性的领域
* 即使集群节点的数量增加，每个节点的负载也不会增加很多，几乎是恒定的。这就允许 Redis Cluster 或者 Consul 集群管理的节点规模能横向扩展到数千个。
##流程
* https://vdn6.vzuu.com/SD/3cd1521c-2355-11eb-905e-ca0d7949bec0.mp4?pkey=AAXH0X6McUpRX-I-niKyng_hrs290N2ACxjKOHeuySxhy1oLVvHrF9437sR3sUXTRbOWDkYVCcRwza0r946XtkZ9&c=avc.0.0&f=mp4&pu=078babd7&bu=078babd7&expiration=1663983851&v=ks6
* Gossip 是周期性的散播消息，假设把周期限定为 1 秒
* 被感染节点随机选择 k 个邻接节点（fan-out）散播消息，这里把 fan-out 设置为 3，每次最多往 3 个节点散播。
* 每次散播消息都选择尚未发送过的节点进行散播
* 收到消息的节点不再往发送节点散播，比如 A -> B，那么 B 进行散播的时候，不再发给 A
* Gossip 过程是异步的，也就是说发消息的节点不会关注对方是否收到，即不等待响应；不管对方有没有收到，它都会每隔 1 秒向周围节点发消息。异步是它的优点，而消息冗余则是它的缺点。
##协议消息
* ping：每个节点都会频繁给其他节点发送ping，其中包含自己的状态还有自己维护的集群元数据，互相通过ping交换元数据；
* pong: 返回ping和meet，包含自己的状态和其他信息，也可以用于信息广播和更新；
* fail: 某个节点判断另一个节点fail之后，就发送fail给其他节点，通知其他节点，指定的节点宕机了。
* meet：某个节点发送meet给新加入的节点，让新节点加入集群中，然后新节点就会开始与其他节点进行通信，不需要发送形成网络的所需的所有CLUSTER MEET命令。发送CLUSTER MEET消息以便每个节点能够达到其他每个节点只需通过一条已知的节点链就够了。由于在心跳包中会交换gossip信息，将会创建节点间缺失的链接。
##优缺点
 * 优点：去中心化、可扩展、容错、一致性收敛、简单，最终一致性协议。
 * 缺点：消息的延迟 ，消息冗余 。
##中间件实现
* redis-cluster，consul 
 